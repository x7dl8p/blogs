---
title: 'When a Big Chunk of the Internet Went Dark'
summary: 'Cloudflare didn’t just slow down, core internal systems crashed outright. What we re witnessing is not something new for this year; we need to understand something bigger.'
image: 'https://cf-assets.www.cloudflare.com/zkvhlag99gkb/9f2k63fiixI2YXDgsnbGq/3c377a6fbd84b5347f814deb6435c476/Cloudflare-Outage-hero-18-nov-2025.png'
publishedAt: '2025-11-19'
---

# What Actually Happened in the Cloudflare Outage on November 18, 2025

So yeah, on November 18 at 11:20 UTC, big chunk of the Internet just… stopped loading. Spotify spun. ChatGPT gave up. Canva froze. Even Downdetector.com went down , which, honestly, felt sarcastic.

But here’s the thing: no one got hacked. No DDoS won. just a size problem.

It was just one config file. A little too big. Pushed a little too fast. And because of how Cloudflare’s network is built, that one file took down *thousands* of machines , one by one, across 330 cities , like dominos wired to the same timer.

Let me walk you through it, the way I pieced it together while waiting for my npm pakage to load during build in PRODUCTION.

---

## First, What Even Is Cloudflare Under the Hood

Most people think of Cloudflare as a CDN or a firewall. Fair. But technically, every HTTP request hits this pipeline:

1. TLS termination (hello, SNI routing),
2. Then into **FL**, short for *Frontline* , their ultra-low-latency proxy layer,
3. Then into **Pingora**, which handles caching, origin fetches, and Workers.

FL is the gatekeeper. It decides: bot or human, cached or fresh, allowed or blocked. To do that fast, it loads *feature files* , tiny config blobs , from a system called **Quicksilver**.

Quicksilver is wild. It’s a globally replicated key-value store, built on LMDB, running a copy in every single PoP. When a config changes, it syncs to every city in seconds. That’s how Cloudflare stays consistent at scale.

But consistency has a cost: if a bad config goes in, it goes *everywhere*.

---

## The Incident: A File That Was Just a Bit Too Big

Here’s how it unfolded, per Cloudflare’s report: a routine permissions update in a ClickHouse cluster "just tightening access controls" caused a query to return duplicate rows, doubling the Bot Management feature file size; FL, enforcing a hard memory limit for safety on high-throughput edge nodes, crashed outright on startup — no fallback, no warning, just failure.

Now here’s the kicker: the file regenerated *every 5 minutes*. And the ClickHouse update was rolling out gradually , so sometimes you got the good file, sometimes the bad one.

That’s why the outage *pulsed*. Services would pop back for 90 seconds, then vanish again. Engineers thought it was a DDoS at first , and honestly thats what it looked like.

Once *all* ClickHouse nodes were updated, every new file was bad. FL stayed down. Global routing collapsed.

By 14:30, they stopped the generator, shoved a known-good file into Quicksilver manually, and restarted FL PoP by PoP. Full recovery by 17:06.

---

## What Broke (Beyond the Obvious)

It wasn’t just websites returning 502s. The failure rippled:

- **Turnstile** didn’t load → no logins anywhere (even Cloudflare’s own dashboard),
- **Workers KV** spiked in errors , because its API gateway runs on FL too,
- **Access** let existing sessions through (they’re cached at edge), but new logins? Nope,
- **Latency went up** not from network load, but because debug tooling was burning CPU trying to annotate the crashes , real “observer effect” vibes.

Affected apps? Almost every major one:
Twitter, ChatGpt, Spotify, Canva, Perplexity, Uber, DoorDash, Claude, and yes, even Downdetector.

All their servers didnt disappear. only The *door* did.

---

## How They Fixed It (and What’s Next)

The fix was surgical:

- Kill the bad config generator,
- Inject a good file into Quicksilver’s ingestion pipeline (bypassing the broken path),
- Roll FL restarts with their zero-downtime tooling.

Long term? They’re adding:

- Pre-flight validation for config size and schema,
- “Last-known-good” fallbacks in FL,
- Better visibility when config propagation is inconsistent across PoPs.

This wasn’t a “they messed up” moment. It was a “scale found the edge of the map” moment. And the fact they published a full post-mortem *while still cleaning up*? That’s respect.

---

## My Take (As a Student Who Runs Everything on Cloudflare)

I have a Old pc at home using Cloudflare Tunnel to expose my lab. I built my final-year project on Workers. I use R2 for backups. I trust this stack , not blindly, but *earnedly*.

When the outage hit, I didn’t rage. I opened `dig +short <my-domain>` and saw it still resolved. So I knew: DNS was up, TLS handshake worked , it was deeper. FL.

And when the blog dropped an hour later? I read it like a textbook. Not to judge, but to learn.

Because here’s the truth: any system this fast, this distributed, this *ambitious* will eventually meet a corner case no test caught.

What matters is: do they own it? Do they explain it? Do they fix it *and* share how?

Cloudflare did all three.

So yeah, the Internet blinked. But it didn’t break.
And I’m still shipping to `*.workers.dev` tonight.
one last thing the image is coming from their servers so if it didnt load, you know what to understand :)

-- mohammad
